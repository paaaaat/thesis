@misc{statista2025,
  author = {{Statista}},
  title  = {Amount of Data Created Worldwide 2010-2025},
  year   = {2025},
  url    = {https://www.statista.com/statistics/871513/worldwide-data-created/}
}


@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Mark and Mishra, Gaurav and Roberts, Adam and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{scao2022bloom,
  title   = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author  = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and others},
  journal = {arXiv preprint arXiv:2211.05100},
  year    = {2022}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and others},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mcculloch1943logical,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  journal = {The Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  year = {1943},
  publisher = {Springer}
}

@article{lewis2020retrieval,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and others},
  journal={arXiv preprint arXiv:2005.11401},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{liu2024understanding,
  title   = {Understanding LLMs: A Comprehensive Overview from Training to Inference},
  author  = {Liu, Yixin and Zhang, Haoyu and Zhang, Zhanpeng and others},
  journal = {arXiv preprint arXiv:2401.02038},
  year    = {2024}
}

@article{hu2021lora,
  title   = {LoRA: Low-Rank Adaptation of Large Language Models},
  author  = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  year    = {2021}
}


@misc{huggingfacecourse,
  author = {Hugging Face},
  title = {The Hugging Face Course, 2022},
  howpublished = "\url{https://huggingface.co/course}",
  year = {2022},
  note = "[Online; accessed <today>]"
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@inproceedings{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  booktitle={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}


@article{rosenblatt1958perceptron,
  title   = {The Perceptron: A probabilistic model for information storage and organization in the brain},
  author  = {Rosenblatt, Frank},
  journal = {Psychological Review},
  volume  = {65},
  number  = {6},
  pages   = {386--408},
  year    = {1958}
}

@book{minsky1969perceptrons,
  title     = {Perceptrons},
  author    = {Minsky, Marvin and Papert, Seymour},
  publisher = {MIT Press},
  year      = {1969}
}

@book{goodfellow2016deep,
  title     = {Deep Learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {MIT Press},
  year      = {2016}
}

@inproceedings{krizhevsky2012imagenet,
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2012}
}

@article{hochreiter1997long,
  title     = {Long short-term memory},
  author    = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal   = {Neural Computation},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  publisher = {MIT Press}
}

@inproceedings{cho2014learning,
  title     = {Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
  author    = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and others},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2014}
}

@inproceedings{mikolov2013distributed,
  title     = {Efficient Estimation of Word Representations in Vector Space},
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2013}
}

@inproceedings{pennington2014glove,
  title     = {GloVe: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2014}
}

@inproceedings{peters2018deep,
  title     = {Deep Contextualized Word Representations},
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and others},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2018}
}

@inproceedings{devlin2019bert,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2019}
}

@article{liu2023promptsurvey,
  title   = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author  = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and others},
  journal = {ACM Computing Surveys},
  volume  = {55},
  number  = {9},
  year    = {2023}
}

@inproceedings{hu2021lora,
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and others},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2021}
}

@article{dettmers2022int8,
  title   = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author  = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal = {arXiv preprint arXiv:2208.07339},
  year    = {2022}
}

@article{hoffmann2022trainingcomputeoptimal,
  title   = {Training Compute-Optimal Large Language Models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022}
}

@article{ji2023surveyhallucination,
  title   = {A Survey of Hallucination in Large Language Models},
  author  = {Ji, Zhijing and Lee, Zheng-Xin and Sun, Chenguang and others},
  journal = {arXiv preprint arXiv:2304.03240},
  year    = {2023}
}

@inproceedings{bender2021parrots,
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author    = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  year      = {2021}
}

@article{weidinger2022ethical,
  title   = {Ethical and Social Risks of Harm from Language Models},
  author  = {Weidinger, Laura and Mellor, Joe and Rauh, Lisa and others},
  journal = {arXiv preprint arXiv:2112.04359},
  year    = {2022}
}