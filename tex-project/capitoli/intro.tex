Over the last decade, the field of artificial intelligence (AI from now on) has experienced significant growth and rapid advancements. Among the different branches of AI, \textit{deep learning} has stood out for its ability to tackle complex tasks that were considered infeasible or extremely challenging. In particular, \textit{Natural Language Processing} (NLP) has gained prominence due to the increasing volume of unstructured and textual data generated and subsequently gathered online (e.g., social media, blogs, scientific articles...); it is estimated that 328.77 million terabytes of data are created daily, 80\% of which are unstructured data. \cite{statista2025} This phenomenon and the rising demand for language-driven applications, such as virtual assistants, sentiment analysis, and automated content generation, has prepared the terrain for the advancement of languange processors to flourish.

Recently, the emergence of \textit{Large Language Models} (LLMs), led by the release of GPT-3 in 2020 \cite{brown2020language}, has taken NLP to a new level. By employing architectures with billions of parameters, these models are capable of producing remarkably fluent, context-aware and human-like text. Despite this progress, several open challenges remain: questions regarding how best to \textit{fine-tune} LLMs, how to incorporate domain-specific knowledge, and how to design effective prompting mechanisms are active areas of research. Additionally, issues related to computational resources, scalability, ethical implications, and potential biases call for continuous investigation.

\textbf{Motivations.} The primary motivation behind this thesis is to explore the practical methods and architectural choices to enable the construction of an LLM-powered application to be effectively adapted for the domain of tourism in Verona, Italy. In particular, the aim is to utilize and demonstrate how techniques such as \textit{prompt engineering}, and \textit{retrieval-augmented generation} (RAG) can facilitate the deployment of state-of-the-art LLMs in real-world applications, including scenarios where computational resources may be limited or costly.

As such, the \textbf{objectives} are:
\begin{itemize}
  \item \textbf{Analyze} the foundational principles and evolution of modern neural network architectures, particularly focusing on the \textit{Transformer} model and its role in Large Language Models.
  \item \textbf{Investigate} the current state of the art for LLMs, highlighting successful applications and the most common methodologies (fine-tuning, prompt engineering, RAG, etc.).
  \item \textbf{Design and implement} a tourism domain application system leveraging a specific open-source LLM (LLaMA 3.1 8B Instruct), showcasing relevant techniques (8-bit quantization, agent-like conversational behavior) for an implicit recommender system for tourists.
  \item \textbf{Evaluate} the system with respect to performance and quality metrics, as well as potential limitations.
\end{itemize}

% \label{sec:context-llms}

From a high-level perspective, Large Language Models represent a class of deep neural netowrks trained on extensive corpora, often comprising billions of parameters. A key turning point in their development was the introduction of the \textit{Transformer} architecture, which provides a self-attention mechanism that allows to improve text tokens processing in a parallel manner during training.

Examples of prominent LLMs include GPT-based models (GPT-3, GPT-3.5, GPT-4), \cite{brown2020language} Google’s PaLM \cite{chowdhery2022palm}, Meta’s LLaMA, \cite{touvron2023llama} and various open-source initiatives such as Bloom. \cite{scao2022bloom} These models have demonstrated remarkable capabilities in language understanding and generation, enabling:
\begin{itemize}
    \item \textbf{Text completion} with human-like fluency.
    \item \textbf{Zero-shot or few-shot generalization} to new tasks with minimal prompt examples.
    \item \textbf{Conversational AI}, powering advanced chatbots and virtual assistants.
\end{itemize}

This thesis will leverage the open-source LLaMA model family as a foundation for an LLM-based chatbot to suggest points of interest to touristi visiting the city of Verona, Italy. By focusing on a quantized 8-bit instruct variant (LLaMA 3.1 8B Instruct), the aim is to highlight practical techniques (RAG, agent AI) that maintain acceptable performance implementing innovative strategies that harness context-awareness and text generation capabilities/question answering of modern LLM models.

% \label{sec:thesis-structure}

The remainder of this document is organized as follows: Chapter 1 presents a deeper dive into the theoretical underpinnings of modern NLP, covering the history of neural networks and the transformation from classic word embeddings to large-scale language models, also detailing the breakthrough Transformer architecture.
Chapter 2 reviews the current landscape of Large Language Models and NLP solutions, exploring established techniques (fine-tuning, prompt engineering, RAG) and highlighting real-world use cases, from chatbots to advanced content generation.
Chapter 3 describes the design and implementation of an application based on LLaMA 3.1 8B Instruct for tourism purposes in the city of Verona, Italy. It covers the decision-making process behind the model choice, 8-bit quantization strategy, prompt engineering, retrieval-augmented generation, and the agent-like conversational framework.
Chapter 4 discusses the evaluation with illustrative examples of the model’s outputs and conversations, along with limitations and areas for improvement.
Finally, Chapter 5 concludes the study with a summary of key findings, limitations, and potential future directions.