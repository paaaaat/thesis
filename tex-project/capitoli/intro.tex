% Over the last decade, the field of artificial intelligence (AI from now on) has experienced significant growth and rapid advancements. Among the different branches of AI, \textit{deep learning} has stood out for its ability to tackle complex tasks that were considered infeasible or extremely challenging. In particular, \textit{Natural Language Processing} (NLP) has gained prominence due to the increasing volume of unstructured and textual data generated and subsequently gathered online (e.g., social media, blogs, scientific articles...); it is estimated that 328.77 million terabytes of data are created daily, 80\% of which are unstructured data. \cite{statista2025} This phenomenon and the rising demand for language-driven applications, such as virtual assistants, sentiment analysis and automated content generation, has prepared the terrain for the advancement of languange processors to flourish.

% Recently, the emergence of \textit{Large Language Models} (LLMs), led by the release of GPT-3 in 2020 \cite{brown2020language}, has taken NLP to a new level. By employing architectures with billions of parameters, these models are capable of producing remarkably fluent, context-aware and human-like text. Despite this progress, several open challenges remain: questions regarding how best to \textit{fine-tune} LLMs, how to incorporate domain-specific knowledge and how to design effective prompting mechanisms are active areas of research. Additionally, issues related to computational resources, scalability, ethical implications and potential biases call for continuous investigation.

% \textbf{Motivations.}

Over the last decade, the field of artificial intelligence (AI from now on) has experienced significant growth and rapid advancements. Among the different branches of AI, \textit{deep learning} has stood out for its ability to tackle complex tasks and in particular \textit{Natural Language Processing} (NLP) has gained prominence due to the emergence of \textit{Large Language Models} (LLMs), led by the release of GPT-3 in 2020. \cite{brown2020language} By employing architectures with billions of parameters, these models are capable of producing remarkably fluent, context-aware and human-like text. Despite this progress, several open challenges remain: questions regarding how best to \textit{fine-tune} LLMs, how to incorporate domain-specific knowledge and how to design effective prompting mechanisms are active areas of research. Additionally, issues related to computational resources, scalability, ethical implications and potential biases call for continuous investigation.

The primary motivation behind this thesis is to explore the practical methods and architectural choices to enable the construction of an LLM-powered application to be effectively adapted for the domain of tourism in Verona, Italy. In particular, the aim is to utilize and demonstrate how techniques such as \textit{prompt engineering}, \textit{retrieval-augmented generation} (RAG) and \textit{Agent AI} can facilitate the deployment of the latest LLMs in real-world applications, including scenarios where computational resources may be limited or costly.

As such, the first objective is to analyze the foundational principles and evolution of modern neural network architectures, particularly focusing on the \textit{Transformer} model and its role in Large Language Models. It will then be investigated the current state-of-the-art for LLMs, highlighting successful applications and the most common methodologies (fine-tuning, prompt engineering, RAG). This will then lead to the main objective of this thesis, that is the design and implementation of a tourism related application leveraging a specific open-source LLM model, showcasing relevant techniques for building an implicit recommender system for tourists. The final objective is to evaluate the system with respect to performance and quality metrics, as well as potential limitations.

% From a high-level perspective, Large Language Models represent a class of deep neural netowrks trained on extensive corpora, often comprising billions of parameters. A key turning point in their development was the introduction of the \textit{Transformer} architecture, which provides a self-attention mechanism that allows to improve text tokens processing in a parallel manner during training.

% Examples of prominent LLMs include GPT-based models (GPT-3, GPT-3.5, GPT-4), \cite{brown2020language} Google’s PaLM \cite{chowdhery2022palm}, Meta’s LLaMA, \cite{touvron2023llama} and various open-source initiatives such as Bloom. \cite{scao2022bloom} These models have demonstrated remarkable capabilities in language understanding and generation, enabling:
% \begin{itemize}
%     \item \textbf{Text completion} with human-like fluency.
%     \item \textbf{Zero-shot or few-shot generalization} to new tasks with minimal prompt examples.
%     \item \textbf{Conversational AI}, powering advanced chatbots and virtual assistants.
% \end{itemize}

This thesis will leverage the open-source LLaMA model family as a foundation for an LLM-based chatbot to suggest points of interest to tourist visiting the city of Verona, Italy. By focusing on a quantized 8-bit instruct variant (LLaMA 3.1 8B Instruct), the aim is to highlight practical techniques (prompt engineering, RAG, Agent AI) that maintain acceptable performance implementing innovative strategies that harness context-awareness and text generation capabilities of modern LLM models.

% \label{sec:thesis-structure}

The remainder of this document is organized as follows: Chapter 1 presents a deep dive into the theoretical underpinnings of modern NLP, covering the history of neural networks, started with the introduction of the \textit{Perceptron} \cite{rosenblatt1958perceptron} for emulating the biological human brain in logic and learning tasks, along with the transformation from classic word embeddings to large-scale language models, also detailing the breakthrough Transformer architecture.
In Chapter 2 a survey of the current landscape of Large Language Models and NLP solutions is proposed, exploring established techniques (fine-tuning, prompt engineering, RAG) to effectively deploy LLM-powered applications and highlighting real-world use cases, from chatbots to advanced content generation.
The description of the design and implementation of an application is covered in Chapter 3, which follows both the logical and physical implementation of an open-source language model, namely LLaMA 3.1 8B Instruct, for tourism purposes in the city of Verona, Italy. It covers the decision-making process behind the model choice, 8-bit quantization strategy, prompt engineering, retrieval-augmented generation and the agent-like conversational framework. The proposed framework will then be displayed in Chapter 4, providing the evaluation along with illustrative examples of the model’s outputs and conversations, also eliciting its limitations and areas for improvement.
Finally, Chapter 5 concludes the study with a summary of key findings, limitations and potential future directions.